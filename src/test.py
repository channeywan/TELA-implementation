import logging
from data.loader import DiskDataLoader
from config.settings import DirConfig, DataConfig, ModelConfig
from algorithms.SCDA import SCDA
import sys
import os
import numpy as np
import pandas as pd
import pickle
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_scda_cluster_centers():
    """æµ‹è¯•SCDAèšç±»æ¨¡å‹çš„ç°‡ä¸­å¿ƒ"""
    print("=" * 60)
    print("SCDAèšç±»æ¨¡å‹ç°‡ä¸­å¿ƒæ£€æŸ¥")
    print("=" * 60)

    # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹
    model_cluster_dir = os.path.join(
        DirConfig.MODEL_DIR, "SCDA", "model_cluster.pkl")
    model_classify_dir = os.path.join(
        DirConfig.MODEL_DIR, "SCDA", "model_classify.pkl")
    scaler_dir = os.path.join(DirConfig.MODEL_DIR, "SCDA", "scaler.pkl")

    print(f"æ¨¡å‹æ–‡ä»¶è·¯å¾„:")
    print(f"  èšç±»æ¨¡å‹: {model_cluster_dir}")
    print(f"  åˆ†ç±»æ¨¡å‹: {model_classify_dir}")
    print(f"  ç¼©æ”¾å™¨: {scaler_dir}")
    print()

    # 2. åˆ›å»ºSCDAå®ä¾‹å¹¶è®­ç»ƒ/åŠ è½½æ¨¡å‹
    scda = SCDA()

    if not os.path.exists(model_cluster_dir):
        print("æ¨¡å‹ä¸å­˜åœ¨ï¼Œå¼€å§‹è®­ç»ƒ...")
        scda.train_model()
        print("è®­ç»ƒå®Œæˆ!")
    else:
        print("å‘ç°å·²å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶")

    # 3. åŠ è½½æ¨¡å‹å’Œæ•°æ®
    try:
        model_cluster, model_classify = scda.load_model()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 4. æ£€æŸ¥èšç±»æ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯
    print(f"\nèšç±»æ¨¡å‹ä¿¡æ¯:")
    print(f"  èšç±»æ•°é‡ K = {model_cluster.n_clusters}")
    print(f"  ç°‡ä¸­å¿ƒå½¢çŠ¶: {model_cluster.cluster_centers_.shape}")
    print(f"  ç‰¹å¾ç»´åº¦: {model_cluster.cluster_centers_.shape[1]}")

    # 5. è¯¦ç»†æ£€æŸ¥æ¯ä¸ªç°‡ä¸­å¿ƒï¼ˆç¼©æ”¾åçš„å€¼ï¼‰
    print(f"\nğŸ“Š ç¼©æ”¾åçš„ç°‡ä¸­å¿ƒåæ ‡:")
    cluster_centers_scaled = model_cluster.cluster_centers_
    for i, center in enumerate(cluster_centers_scaled):
        print(
            f"  ç°‡ {i}: [{center[0]:.6f}, {center[1]:.6f}, {center[2]:.6f}, {center[3]:.6f}]")

    # 6. æ£€æŸ¥ç°‡ä¸­å¿ƒæ˜¯å¦å®Œå…¨ç›¸åŒ
    print(f"\nğŸ” ç°‡ä¸­å¿ƒç›¸ä¼¼æ€§æ£€æŸ¥:")
    centers_are_identical = True
    for i in range(len(cluster_centers_scaled)):
        for j in range(i+1, len(cluster_centers_scaled)):
            diff = np.abs(
                cluster_centers_scaled[i] - cluster_centers_scaled[j])
            max_diff = np.max(diff)
            print(f"  ç°‡{i} vs ç°‡{j}: æœ€å¤§å·®å¼‚ = {max_diff:.8f}")
            if max_diff > 1e-10:  # è€ƒè™‘æµ®ç‚¹æ•°ç²¾åº¦
                centers_are_identical = False

    if centers_are_identical:
        print("  âš ï¸  è­¦å‘Š: æ‰€æœ‰ç°‡ä¸­å¿ƒå®Œå…¨ç›¸åŒ!")
    else:
        print("  âœ… ç°‡ä¸­å¿ƒå­˜åœ¨å·®å¼‚")

    # 7. åç¼©æ”¾åˆ°åŸå§‹å°ºåº¦å¹¶æ£€æŸ¥
    if hasattr(scda, 'scaler') and scda.scaler is not None:
        print(f"\nğŸ“Š åŸå§‹å°ºåº¦çš„ç°‡ä¸­å¿ƒåæ ‡:")
        cluster_centers_original = scda.scaler.inverse_transform(
            cluster_centers_scaled)

        for i, center in enumerate(cluster_centers_original):
            print(
                f"  ç°‡ {i}: [avg_rbw={center[0]:.2f}, avg_wbw={center[1]:.2f}, peak_rbw={center[2]:.2f}, peak_wbw={center[3]:.2f}]")

        # æ£€æŸ¥åŸå§‹å°ºåº¦ä¸‹çš„å·®å¼‚
        print(f"\nğŸ” åŸå§‹å°ºåº¦ç°‡ä¸­å¿ƒç›¸ä¼¼æ€§æ£€æŸ¥:")
        original_centers_identical = True
        for i in range(len(cluster_centers_original)):
            for j in range(i+1, len(cluster_centers_original)):
                diff = np.abs(
                    cluster_centers_original[i] - cluster_centers_original[j])
                max_diff = np.max(diff)
                print(f"  ç°‡{i} vs ç°‡{j}: æœ€å¤§å·®å¼‚ = {max_diff:.4f}")
                if max_diff > 0.01:  # åŸå§‹å°ºåº¦ä¸‹çš„åˆç†é˜ˆå€¼
                    original_centers_identical = False

        if original_centers_identical:
            print("  âš ï¸  è­¦å‘Š: åŸå§‹å°ºåº¦ä¸‹æ‰€æœ‰ç°‡ä¸­å¿ƒä¹Ÿå‡ ä¹ç›¸åŒ!")
        else:
            print("  âœ… åŸå§‹å°ºåº¦ä¸‹ç°‡ä¸­å¿ƒå­˜åœ¨åˆç†å·®å¼‚")
    else:
        print("  âŒ ç¼©æ”¾å™¨ä¸å­˜åœ¨ï¼Œæ— æ³•åç¼©æ”¾")

    # 8. åŠ è½½è®­ç»ƒæ•°æ®è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ
    print(f"\nğŸ“ˆ è®­ç»ƒæ•°æ®åˆ†æ:")
    try:
        loader = DiskDataLoader()
        items_train = loader.load_items(
            type="both",
            cluster_index_list=DataConfig.CLUSTER_INDEX_LIST_TRAIN,
            purpose="train"
        )

        if len(items_train) > 0:
            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œåˆ†æ
            items_df = pd.DataFrame(items_train, columns=[
                "disk_ID", "disk_capacity", "disk_if_local", "disk_attr",
                "disk_type", "disk_if_VIP", "disk_pay", "vm_cpu", "vm_mem",
                "avg_rbw", "avg_wbw", "peak_rbw", "peak_wbw", "timestamp_num",
                "burst_label", "cluster_index"
            ])

            # åˆ†æèšç±»ç‰¹å¾çš„åˆ†å¸ƒ
            cluster_features = ["avg_rbw", "avg_wbw", "peak_rbw", "peak_wbw"]
            print(f"  è®­ç»ƒæ•°æ®æ ·æœ¬æ•°: {len(items_df)}")
            print(f"  èšç±»ç‰¹å¾ç»Ÿè®¡:")

            for feature in cluster_features:
                values = items_df[feature].astype(float)
                print(f"    {feature}: min={values.min():.2f}, max={values.max():.2f}, "
                      f"mean={values.mean():.2f}, std={values.std():.2f}")

            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰è¶³å¤Ÿçš„å˜å¼‚æ€§
            feature_data = items_df[cluster_features].astype(float)
            print(f"\n  æ•°æ®å˜å¼‚æ€§æ£€æŸ¥:")
            print(f"    ç‰¹å¾é—´ç›¸å…³æ€§:")
            correlation_matrix = feature_data.corr()
            print(correlation_matrix.round(3))

            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ•°æ®ç‚¹éƒ½ç›¸åŒ
            unique_combinations = feature_data.drop_duplicates()
            print(
                f"    å”¯ä¸€çš„ç‰¹å¾ç»„åˆæ•°: {len(unique_combinations)} / {len(feature_data)}")

            if len(unique_combinations) < ModelConfig.SCDA_CLUSTER_K:
                print(
                    f"  âš ï¸  è­¦å‘Š: å”¯ä¸€ç‰¹å¾ç»„åˆæ•°({len(unique_combinations)}) < èšç±»æ•°({ModelConfig.SCDA_CLUSTER_K})")
                print("     è¿™å¯èƒ½å¯¼è‡´èšç±»ä¸­å¿ƒé‡å !")

        else:
            print("  âŒ è®­ç»ƒæ•°æ®ä¸ºç©º")

    except Exception as e:
        print(f"  âŒ è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")

    # 9. æä¾›è¯Šæ–­å»ºè®®
    print(f"\nğŸ’¡ è¯Šæ–­å»ºè®®:")
    if centers_are_identical:
        print("  1. æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦æœ‰è¶³å¤Ÿçš„å˜å¼‚æ€§")
        print("  2. è€ƒè™‘å‡å°‘èšç±»æ•°é‡K")
        print("  3. æ£€æŸ¥ç‰¹å¾é¢„å¤„ç†æ˜¯å¦æ­£ç¡®")
        print("  4. å°è¯•ä½¿ç”¨ä¸åŒçš„èšç±»ç®—æ³•å‚æ•°")
        print("  5. æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨å¼‚å¸¸å€¼æˆ–æ•°æ®è´¨é‡é—®é¢˜")
    else:
        print("  âœ… èšç±»æ¨¡å‹çœ‹èµ·æ¥æ­£å¸¸")

    print("=" * 60)


def test_prediction_pipeline():
    """æµ‹è¯•å®Œæ•´çš„é¢„æµ‹æµç¨‹"""
    print("\n" + "=" * 60)
    print("SCDAé¢„æµ‹æµç¨‹æµ‹è¯•")
    print("=" * 60)

    try:
        scda = SCDA()

        # åŠ è½½é¢„æµ‹æ•°æ®
        loader = DiskDataLoader()
        items_predict = loader.load_items(
            type="both",
            cluster_index_list=DataConfig.CLUSTER_INDEX_LIST_PREDICT,
            purpose="train"
        )

        if len(items_predict) > 0:
            items_df = pd.DataFrame(items_predict, columns=[
                "disk_ID", "disk_capacity", "disk_if_local", "disk_attr",
                "disk_type", "disk_if_VIP", "disk_pay", "vm_cpu", "vm_mem",
                "avg_rbw", "avg_wbw", "peak_rbw", "peak_wbw", "timestamp_num",
                "burst_label", "cluster_index"
            ])

            print(f"é¢„æµ‹æ•°æ®æ ·æœ¬æ•°: {len(items_df)}")

            # é€‰æ‹©å‰5ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
            test_samples = items_df.head(5)

            print(f"\næµ‹è¯•æ ·æœ¬çš„çœŸå®å€¼:")
            for idx, (_, row) in enumerate(test_samples.iterrows()):
                print(
                    f"  æ ·æœ¬{idx}: avg_rbw={row['avg_rbw']:.2f}, avg_wbw={row['avg_wbw']:.2f}")

            # è¿›è¡Œé¢„æµ‹
            predictions = scda.predict(test_samples)

            print(f"\né¢„æµ‹ç»“æœ:")
            for idx, (_, row) in enumerate(predictions.head(5).iterrows()):
                print(
                    f"  æ ·æœ¬{idx}: pre_avg_rbw={row['pre_avg_rbw']:.6f}, pre_avg_wbw={row['pre_avg_wbw']:.6f}")

            # è®¡ç®—é¢„æµ‹è¯¯å·®
            print(f"\né¢„æµ‹è¯¯å·®åˆ†æ:")
            for idx in range(min(5, len(predictions))):
                true_rbw = float(test_samples.iloc[idx]['avg_rbw'])
                true_wbw = float(test_samples.iloc[idx]['avg_wbw'])
                pred_rbw = float(predictions.iloc[idx]['pre_avg_rbw'])
                pred_wbw = float(predictions.iloc[idx]['pre_avg_wbw'])

                error_rbw = abs(true_rbw - pred_rbw)
                error_wbw = abs(true_wbw - pred_wbw)
                relative_error_rbw = error_rbw / max(true_rbw, 1e-6) * 100
                relative_error_wbw = error_wbw / max(true_wbw, 1e-6) * 100

                print(f"  æ ·æœ¬{idx}:")
                print(
                    f"    RBWè¯¯å·®: {error_rbw:.4f} (ç›¸å¯¹è¯¯å·®: {relative_error_rbw:.1f}%)")
                print(
                    f"    WBWè¯¯å·®: {error_wbw:.4f} (ç›¸å¯¹è¯¯å·®: {relative_error_wbw:.1f}%)")

        else:
            print("âŒ é¢„æµ‹æ•°æ®ä¸ºç©º")

    except Exception as e:
        print(f"âŒ é¢„æµ‹æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    DirConfig.ensure_dirs()

    # è¿è¡Œæµ‹è¯•
    test_scda_cluster_centers()
    test_prediction_pipeline()
